{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1: Reccurent neuronal network: IMBD sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our goal here will be to be able to predict if a comment is positive or negative based only on the text comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we importe all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import keras\n",
    "import re\n",
    "import tarfile\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We preview the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "english_stops = set(stopwords.words('english'))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we do not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('IMDB Dataset.csv')\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "21437    [although, movie, slow, dream, like, almost, m...\n",
      "5001     [this, film, typical, hollywood, fare, though,...\n",
      "47386    [sarah, silverman, dangerous, bitch, she, beau...\n",
      "45303    [an, amazing, film, i, seen, i, already, want,...\n",
      "3728     [if, film, noir, noir, must, french, glacially...\n",
      "                               ...                        \n",
      "24886    [this, movie, advertised, radio, television, m...\n",
      "19639    [found, one, video, store, rented, it, one, qu...\n",
      "48731    [i, know, neighborhoods, folks, wrote, rave, r...\n",
      "3921     [i, think, lion, king, one, best, sequels, eve...\n",
      "42828    [for, love, god, please, see, movie, its, wast...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "21437    0\n",
      "5001     1\n",
      "47386    1\n",
      "45303    1\n",
      "3728     0\n",
      "        ..\n",
      "24886    0\n",
      "19639    1\n",
      "48731    0\n",
      "3921     1\n",
      "42828    0\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "Test Set\n",
      "43216    [ocean, twelve, plain, stupid, bad, nothing, c...\n",
      "20978    [broadway, film, actor, turned, director, john...\n",
      "47247    [this, got, unique, twists, two, genres, ever,...\n",
      "48237    [she, michael, jordanthink, marvelous, nba, pl...\n",
      "8528     [although, remade, several, times, movie, clas...\n",
      "                               ...                        \n",
      "23330    [this, one, underrated, masterpieces, time, op...\n",
      "44506    [i, found, film, rather, brilliant, initially,...\n",
      "47450    [terrific, movie, if, watch, yet, must, watch,...\n",
      "45433    [this, installment, much, makes, cia, look, li...\n",
      "39606    [this, movie, biggest, piece, garbage, i, seen...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "43216    0\n",
      "20978    1\n",
      "47247    1\n",
      "48237    1\n",
      "8528     1\n",
      "        ..\n",
      "23330    1\n",
      "44506    1\n",
      "47450    1\n",
      "45433    1\n",
      "39606    0\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(y_train, '\\n')\n",
    "print('Test Set')\n",
    "print(x_test, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting the maximum review length\n",
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Pad/Truncate Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[  165     3   466 ...     0     0     0]\n",
      " [    8     4   714 ...     0     0     0]\n",
      " [ 2551  8465  1638 ...     0     0     0]\n",
      " ...\n",
      " [    1    48 15907 ... 21511     0     0]\n",
      " [    1    31  2829 ...     0     0     0]\n",
      " [  204    40   404 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[2912 4115  943 ...    0    0    0]\n",
      " [2356    4  180 ...   28 1030 3091]\n",
      " [   8   99  867 ...    0    0    0]\n",
      " ...\n",
      " [1234    3   55 ...    0    0    0]\n",
      " [   8 3847   17 ...    0    0    0]\n",
      " [   8    3 1018 ...    0    0    0]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Architecture/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 130, 32)           2956864   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,981,761\n",
      "Trainable params: 2,981,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 25s 75ms/step - loss: 0.6001 - accuracy: 0.6186\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.74630, saving model to models\\LSTM.h5\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.2101 - accuracy: 0.9264\n",
      "\n",
      "Epoch 00002: accuracy improved from 0.74630 to 0.92385, saving model to models\\LSTM.h5\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1164 - accuracy: 0.9632\n",
      "\n",
      "Epoch 00003: accuracy improved from 0.92385 to 0.95985, saving model to models\\LSTM.h5\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 0.0755 - accuracy: 0.9803\n",
      "\n",
      "Epoch 00004: accuracy improved from 0.95985 to 0.97723, saving model to models\\LSTM.h5\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.0510 - accuracy: 0.9869\n",
      "\n",
      "Epoch 00005: accuracy improved from 0.97723 to 0.98480, saving model to models\\LSTM.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25d1058b4c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Megaport\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 8672\n",
      "Wrong Prediction: 1328\n",
      "Accuracy: 86.72\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_test, batch_size = 128)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('models/LSTM.h5')\n",
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', review)\n",
    "print('Cleaned: ', review)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in english_stops]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [filtered.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the sentiment of short textual comments about movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2: Text Classification: the Ohsumed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For this second exercice we are going to realize a text classifier using deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To beginning, we are going to use the dataset that contains 20 000 articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To begin, we are just going to understand how the data works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse your data\n",
    "def get_info(path: str):\n",
    "    data = list(os.walk(path))[1:]\n",
    "    files = []\n",
    "    categories = []\n",
    "    textes = []\n",
    "    for d in data:\n",
    "        folder_name = d[0]\n",
    "        for file in d[2]:\n",
    "            s = folder_name.split('/')[-1].split(\"\\\\\")[1]\n",
    "            categories.append(s) # the tags of our model\n",
    "            files.append((folder_name.split('/')[-1], os.path.join(folder_name, file)))\n",
    "\n",
    "    d = defaultdict(int)\n",
    "    texts = defaultdict(list)\n",
    "    for (cate, file) in files:\n",
    "        with open(file, 'r') as outfile:\n",
    "            text = outfile.read()\n",
    "            texts[cate].append(text)\n",
    "            textes.append(text)\n",
    "            words = text_to_word_sequence(text)\n",
    "            for word in words:\n",
    "                d[word] += 1\n",
    "    data = pd.DataFrame(data={'Categories' : categories, 'Textes' : textes}) # a dataFrame with the categorie corresponding to its text description\n",
    "    words = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    return (data, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory : C:\\Users\\Megaport\\OneDrive\\Bureau\\IA\\TP_1_et_2_IA\n"
     ]
    }
   ],
   "source": [
    "# First we import the dataset of 20 000\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory :', cwd)\n",
    "data_dir_training = cwd + \"/ohsumed-first-20000-docs/training\"\n",
    "data_dir_test = cwd + \"/ohsumed-first-20000-docs/test\"\n",
    "data_train, words = get_info(data_dir_training)\n",
    "data_test, words_test = get_info(data_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Textes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C01</td>\n",
       "      <td>Augmentation mentoplasty using Mersilene mesh....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C01</td>\n",
       "      <td>Multiple intracranial mucoceles associated wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C01</td>\n",
       "      <td>Replacement of an aortic valve cusp after neon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C01</td>\n",
       "      <td>The value of indium 111 leukocyte scanning in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C01</td>\n",
       "      <td>Febrile infants less than eight weeks old. Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10428</th>\n",
       "      <td>C23</td>\n",
       "      <td>Afferent nipple valve malfunction caused by an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10429</th>\n",
       "      <td>C23</td>\n",
       "      <td>Extracorporeal shock wave lithotripsy in combi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10430</th>\n",
       "      <td>C23</td>\n",
       "      <td>Tamm-Horsfall autoantibodies in interstitial c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10431</th>\n",
       "      <td>C23</td>\n",
       "      <td>Results of contemporary radical cystectomy for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432</th>\n",
       "      <td>C23</td>\n",
       "      <td>Total bladder replacement using detubularized ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10433 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Categories                                             Textes\n",
       "0            C01  Augmentation mentoplasty using Mersilene mesh....\n",
       "1            C01  Multiple intracranial mucoceles associated wit...\n",
       "2            C01  Replacement of an aortic valve cusp after neon...\n",
       "3            C01  The value of indium 111 leukocyte scanning in ...\n",
       "4            C01  Febrile infants less than eight weeks old. Pre...\n",
       "...          ...                                                ...\n",
       "10428        C23  Afferent nipple valve malfunction caused by an...\n",
       "10429        C23  Extracorporeal shock wave lithotripsy in combi...\n",
       "10430        C23  Tamm-Horsfall autoantibodies in interstitial c...\n",
       "10431        C23  Results of contemporary radical cystectomy for...\n",
       "10432        C23  Total bladder replacement using detubularized ...\n",
       "\n",
       "[10433 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Textes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C01</td>\n",
       "      <td>Haemophilus influenzae meningitis with prolong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C01</td>\n",
       "      <td>Mucosal intussusception to avoid ascending cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C01</td>\n",
       "      <td>Gastrointestinal function and structure in HIV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C01</td>\n",
       "      <td>Epidemiology in bone and joint infection.\\n Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C01</td>\n",
       "      <td>The diabetic foot. Soft tissue and bone infect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12728</th>\n",
       "      <td>C23</td>\n",
       "      <td>Interruption of professional and home activity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12729</th>\n",
       "      <td>C23</td>\n",
       "      <td>Effect of thiopental on neurologic outcome fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12730</th>\n",
       "      <td>C23</td>\n",
       "      <td>Recovery profile after desflurane-nitrous oxid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12731</th>\n",
       "      <td>C23</td>\n",
       "      <td>Postoperative myocardial ischemia in patients ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12732</th>\n",
       "      <td>C23</td>\n",
       "      <td>Enhanced mobilization of intracellular Ca2+ in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12733 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Categories                                             Textes\n",
       "0            C01  Haemophilus influenzae meningitis with prolong...\n",
       "1            C01  Mucosal intussusception to avoid ascending cho...\n",
       "2            C01  Gastrointestinal function and structure in HIV...\n",
       "3            C01  Epidemiology in bone and joint infection.\\n Th...\n",
       "4            C01  The diabetic foot. Soft tissue and bone infect...\n",
       "...          ...                                                ...\n",
       "12728        C23  Interruption of professional and home activity...\n",
       "12729        C23  Effect of thiopental on neurologic outcome fol...\n",
       "12730        C23  Recovery profile after desflurane-nitrous oxid...\n",
       "12731        C23  Postoperative myocardial ischemia in patients ...\n",
       "12732        C23  Enhanced mobilization of intracellular Ca2+ in...\n",
       "\n",
       "[12733 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        words  percentage\n",
      "0                         the    4.573823\n",
      "1                          of    4.545638\n",
      "2                         and    3.080502\n",
      "3                          in    2.964911\n",
      "4                          to    1.660441\n",
      "...                       ...         ...\n",
      "30852                   dtssp    0.000054\n",
      "30853                opossums    0.000054\n",
      "30854               131iodine    0.000054\n",
      "30855  intraperitonealization    0.000054\n",
      "30856                    ball    0.000054\n",
      "\n",
      "[30857 rows x 2 columns]\n",
      "                  words  percentage\n",
      "0                    of    5.738498\n",
      "1                   the    5.660075\n",
      "2                   and    3.855590\n",
      "3                    in    3.711868\n",
      "4                  with    2.071651\n",
      "...                 ...         ...\n",
      "34365           greeted    0.000054\n",
      "34366          tempered    0.000054\n",
      "34367              earn    0.000054\n",
      "34368  nonmitochondrial    0.000054\n",
      "34369           saponin    0.000054\n",
      "\n",
      "[34370 rows x 2 columns]\n",
      "10433\n",
      "12733\n"
     ]
    }
   ],
   "source": [
    "valeurMax = 0\n",
    "for i in range(len(words)):\n",
    "    valeurMax += words[i][1]\n",
    "listeMots = []\n",
    "listePourcentage = []\n",
    "for i in range(len(words)):\n",
    "    listeMots.append(words[i][0])\n",
    "    listePourcentage.append(((words[i][1]/valeurMax) * 100))\n",
    "d = {'words': listeMots, 'percentage': listePourcentage}\n",
    "df = pd.DataFrame(data=d)\n",
    "listeMots = []\n",
    "listePourcentage = []\n",
    "for i in range(len(words_test)):\n",
    "    listeMots.append(words_test[i][0])\n",
    "    listePourcentage.append(((words_test[i][1]/valeurMax) * 100))\n",
    "d = {'words': listeMots, 'percentage': listePourcentage}\n",
    "dt = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "print(dt)\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are now going to prepocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data trains\n",
      "0        [augmentation, mentoplasty, using, mersilene, ...\n",
      "1        [multiple, intracranial, mucoceles, associated...\n",
      "2        [replacement, aortic, valve, cusp, neonatal, e...\n",
      "3        [the, value, indium, leukocyte, scanning, eval...\n",
      "4        [febrile, infants, less, eight, weeks, old, pr...\n",
      "                               ...                        \n",
      "10428    [afferent, nipple, valve, malfunction, caused,...\n",
      "10429    [extracorporeal, shock, wave, lithotripsy, com...\n",
      "10430    [tamm, horsfall, autoantibodies, interstitial,...\n",
      "10431    [results, contemporary, radical, cystectomy, i...\n",
      "10432    [total, bladder, replacement, using, detubular...\n",
      "Name: Textes, Length: 10433, dtype: object \n",
      "\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "         ..\n",
      "10428    23\n",
      "10429    23\n",
      "10430    23\n",
      "10431    23\n",
      "10432    23\n",
      "Name: Categories, Length: 10433, dtype: int64 \n",
      "\n",
      "Data test\n",
      "0        [haemophilus, influenzae, meningitis, prolonge...\n",
      "1        [mucosal, intussusception, avoid, ascending, c...\n",
      "2        [gastrointestinal, function, structure, hiv, p...\n",
      "3        [epidemiology, bone, joint, infection, the, st...\n",
      "4        [the, diabetic, foot, soft, tissue, bone, infe...\n",
      "                               ...                        \n",
      "12728    [interruption, professional, home, activity, l...\n",
      "12729    [effect, thiopental, neurologic, outcome, foll...\n",
      "12730    [recovery, profile, desflurane, nitrous, oxide...\n",
      "12731    [postoperative, myocardial, ischemia, patients...\n",
      "12732    [enhanced, mobilization, intracellular, ca, in...\n",
      "Name: Textes, Length: 12733, dtype: object \n",
      "\n",
      "0         1\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "         ..\n",
      "12728    23\n",
      "12729    23\n",
      "12730    23\n",
      "12731    23\n",
      "12732    23\n",
      "Name: Categories, Length: 12733, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data):\n",
    "    x_data = data['Textes']      # Text/Input\n",
    "    y_data = data['Categories']      # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('C01', 1)\n",
    "    y_data = y_data.replace('C02', 2)\n",
    "    y_data = y_data.replace('C03', 3)\n",
    "    y_data = y_data.replace('C04', 4)\n",
    "    y_data = y_data.replace('C05', 5)\n",
    "    y_data = y_data.replace('C06', 6)\n",
    "    y_data = y_data.replace('C07', 7)\n",
    "    y_data = y_data.replace('C08', 8)\n",
    "    y_data = y_data.replace('C09', 9)\n",
    "    y_data = y_data.replace('C10', 10)\n",
    "    y_data = y_data.replace('C11', 11)\n",
    "    y_data = y_data.replace('C12', 12)\n",
    "    y_data = y_data.replace('C13', 13)\n",
    "    y_data = y_data.replace('C14', 14)\n",
    "    y_data = y_data.replace('C15', 15)\n",
    "    y_data = y_data.replace('C16', 16)\n",
    "    y_data = y_data.replace('C17', 17)\n",
    "    y_data = y_data.replace('C18', 18)\n",
    "    y_data = y_data.replace('C19', 19)\n",
    "    y_data = y_data.replace('C20', 20)\n",
    "    y_data = y_data.replace('C21', 21)\n",
    "    y_data = y_data.replace('C22', 22)\n",
    "    y_data = y_data.replace('C23', 23)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_train, y_train = load_dataset(data_train)\n",
    "x_test, y_test = load_dataset(data_test)\n",
    "\n",
    "print('Data trains')\n",
    "print(x_train, '\\n')\n",
    "print(y_train, '\\n')\n",
    "print('Data test')\n",
    "print(x_test, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to tokenise and Pad/Truncate Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now, we can create our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we achieved it on a small dataset, we are going to do it on a more wide dataset of 50 000 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
